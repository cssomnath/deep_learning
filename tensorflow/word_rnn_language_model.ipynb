{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow code to build single cell word level RNN language model\n",
    "\n",
    "Most of the available tensorflow RNN language model codes in the github work on a single continuous stream of text tokens. But in many real life scenarios the corpus consists of set of documents. In such cases, document boundaries needs to be honored while creating the training dataset. For example, we should *not* create a training sample for the RNN combining the last token of a document and the first token of the next document.\n",
    "\n",
    "This code \n",
    "- Takes a set of sentences (documents) split by new line as input\n",
    "- Creates training batches that do not cross the sentence boundaries\n",
    "- Trains a simple language model\n",
    "- Demonstrates code to predict the next word from a given sequence of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import itertools\n",
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Reader and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UNKNOWN_TOKEN = \"UNKNOWN_TOKEN\"\n",
    "SENTENCE_START_TOKEN = \"SENTENCE_START\"\n",
    "SENTENCE_END_TOKEN = \"SENTENCE_END\"\n",
    "\n",
    "def cleanup_sentence(sent):\n",
    "    sent = sent.strip()\n",
    "    # discard single token sentence\n",
    "    if len(sent) <= 1:\n",
    "        return \"\"\n",
    "    \n",
    "    # Unquote the sentence\n",
    "    if sent[0] == '\"' and sent[-1] == '\"':\n",
    "        return cleanup_sentence(sent[1:-1])\n",
    "    return sent\n",
    "\n",
    "def read_sentences(file_name):\n",
    "    print \"Reading sentences from \" + file_name\n",
    "    sentences = []\n",
    "    with open(file_name, 'rb') as f:\n",
    "        for line in f:\n",
    "            sent = line.strip().decode('utf-8').lower()\n",
    "            sent = cleanup_sentence(sent)\n",
    "            if sent:\n",
    "                sentences.append(sent)\n",
    "\n",
    "    print \"Parsed %d sentences.\" % (len(sentences))    \n",
    "    return sentences\n",
    "\n",
    "def tokenize_sentences(sentences):\n",
    "    max_sent_len = 0\n",
    "    tokenized_sentences = []\n",
    "    for sent in sentences:\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sent))\n",
    "        max_sent_len = max(max_sent_len, len(sent))\n",
    "\n",
    "    print \"Maximum length of a sentence \", max_sent_len\n",
    "    return tokenized_sentences\n",
    "\n",
    "def tokenize_sentences_from_file(file_name):\n",
    "    sentences = read_sentences(file_name)\n",
    "    return tokenize_sentences(sentences) \n",
    "        \n",
    "def create_dictionary(tokenized_sentences, vocabulary_size):\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    print \"Found %d unique words tokens.\" % len(word_freq.items())\n",
    "\n",
    "    vocab = word_freq.most_common(vocabulary_size - 3)\n",
    "    index_to_word = [x[0] for x in vocab]\n",
    "    \n",
    "    index_to_word.extend([UNKNOWN_TOKEN, SENTENCE_START_TOKEN, SENTENCE_END_TOKEN])\n",
    "    word_to_index = dict([(w, i) for i,w in enumerate(index_to_word)])\n",
    "    \n",
    "    return word_to_index, index_to_word\n",
    "        \n",
    "class WordDictionary(object):\n",
    "    def __init__(self, tokenized_sentences, vocabulary_size=100000):\n",
    "        self._word_to_index, self._index_to_word = create_dictionary(\n",
    "            tokenized_sentences, vocabulary_size)\n",
    "    \n",
    "    def words_to_indices(self, tokenized_sentences):\n",
    "        x = np.asarray([\n",
    "                [self._word_to_index.get(w, self._word_to_index[UNKNOWN_TOKEN]) for w in sent]\n",
    "                for sent in tokenized_sentences])\n",
    "        return x\n",
    "        \n",
    "    def indices_to_words(self, x):\n",
    "        return [self._index_to_word[w] for w in x]\n",
    "    \n",
    "    def get_word_index(self, word):\n",
    "        return self._word_to_index[word]\n",
    "    \n",
    "    def get_index_word(self, index):\n",
    "        return self._index_to_word[index]\n",
    "    \n",
    "    def vocabulary_size(self):\n",
    "        return len(self._word_to_index)\n",
    "    \n",
    "    def word_from_probs(self, probabilities):\n",
    "        return [self._index_to_word[i] for i in np.argmax(probabilities, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sentences from ../data/product_titles_100k.txt\n",
      "Parsed 100000 sentences.\n",
      "Maximum length of a sentence  1418\n",
      "Found 213854 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "train_file = \"../data/product_titles_100k.txt\"\n",
    "tokenized_sentences = tokenize_sentences_from_file(train_file)\n",
    "word_dict = WordDictionary(tokenized_sentences)\n",
    "sentences = word_dict.words_to_indices(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Generator\n",
    "- Adds SENTENCE_START and SENTENCE_END markers\n",
    "- Creates batches of numpy matrix to feed into the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, sentences, word_dict, \n",
    "                 batch_size=64, num_unrollings=3,\n",
    "                 max_sent_len=20):\n",
    "\n",
    "        self._word_dict = word_dict\n",
    "        self._sentences = sentences\n",
    "        self._batch_size = batch_size\n",
    "        self._num_sentences = len(sentences)\n",
    "        self._num_unrollings = num_unrollings\n",
    "\n",
    "        self._batch_cursor = 0\n",
    "        self._unrolling_cursor = 0\n",
    "        self._vocabulary_size = word_dict.vocabulary_size()\n",
    "        \n",
    "        self._max_sent_len = max_sent_len\n",
    "        self._batch_sent_len = max_sent_len\n",
    "        \n",
    "        self._sent_start = word_dict.get_word_index(SENTENCE_START_TOKEN)\n",
    "        self._sent_end = word_dict.get_word_index(SENTENCE_END_TOKEN)\n",
    "\n",
    "        # 2 additional tokens for start and end\n",
    "        self._data = np.zeros((batch_size, max_sent_len + 2))\n",
    "        self._load_next_batch_data()\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _load_next_batch_data(self):\n",
    "        # Add sentence start and end markers\n",
    "        self._data.fill(self._sent_end)\n",
    "        self._data[:, 0].fill(self._sent_start)\n",
    "\n",
    "        batch_sent_len = 0\n",
    "        i = self._batch_cursor\n",
    "        for j in range(self._batch_size):\n",
    "            sent = self._sentences[(i + j) % self._num_sentences]\n",
    "            k = min(self._max_sent_len, len(sent))\n",
    "            self._data[j, 1:k+1] = sent[:k]\n",
    "            batch_sent_len = max(k, batch_sent_len)\n",
    "\n",
    "        self._batch_sent_len = batch_sent_len + 2\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        x = self._data[:, self._unrolling_cursor]\n",
    "        self._unrolling_cursor += 1\n",
    "        return (x[:, None] == np.arange(self._vocabulary_size)).astype(np.float32)\n",
    "        \n",
    "    def next(self):\n",
    "        if self._unrolling_cursor + self._num_unrollings > self._batch_sent_len:\n",
    "            self._unrolling_cursor = 0\n",
    "            self._batch_cursor = (\n",
    "                self._batch_cursor + self._batch_size) % self._num_sentences\n",
    "            \n",
    "            self._load_next_batch_data()\n",
    "            self._last_batch = self._next_batch()\n",
    "                \n",
    "        sentence_start = False\n",
    "        if self._unrolling_cursor == 1:\n",
    "            sentence_start = True\n",
    "\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "\n",
    "        return sentence_start, batches\n",
    "    \n",
    "    def batches2sentences(self, batches):\n",
    "        s = [''] * batches[0].shape[0]\n",
    "        for b in batches:\n",
    "            s = [\" \".join(w) for w in zip(s, self._word_dict.word_from_probs(b))]\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[u' SENTENCE_START UNKNOWN_TOKEN bass UNKNOWN_TOKEN', u' SENTENCE_START UNKNOWN_TOKEN give it']\n"
     ]
    }
   ],
   "source": [
    "bg = BatchGenerator(sentences, word_dict, batch_size=2)\n",
    "sentence_start, batches = bg.next()\n",
    "print sentence_start\n",
    "print bg.batches2sentences(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \n",
    "    def __init__(self, vocabulary_size, state_size = 64):        \n",
    "        self._U = tf.Variable(tf.truncated_normal([vocabulary_size, state_size], -0.1, 0.1))\n",
    "        self._W = tf.Variable(tf.truncated_normal([state_size, state_size], -0.1, 0.1))\n",
    "        self._V = tf.Variable(tf.truncated_normal([state_size, vocabulary_size], -0.1, 0.1))\n",
    "        self._state_size = state_size\n",
    "    \n",
    "    def state_size(self):\n",
    "        return self._state_size\n",
    "\n",
    "    def rnn_cell(self, i, s):\n",
    "        s_t = tf.tanh(tf.matmul(i, self._U) + tf.matmul(s, self._W))\n",
    "        o_t = tf.matmul(s_t, self._V)\n",
    "        return o_t, s_t\n",
    "    \n",
    "    def forward_propagation(self, batches, saved_state):\n",
    "        outputs = []\n",
    "        state = saved_state\n",
    "        for b in batches:\n",
    "            output, state = self.rnn_cell(b, state)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        saved_state.assign(state)\n",
    "        return outputs\n",
    "    \n",
    "    def loss(self, batches_x, batches_y, saved_state):\n",
    "        outputs = self.forward_propagation(batches_x, saved_state)\n",
    "        \n",
    "        with tf.control_dependencies([saved_state]):\n",
    "            logits = tf.concat(0, outputs)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits, batches_y))\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the RNN network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "bg = BatchGenerator(sentences, word_dict, batch_size=batch_size)\n",
    "_, batches = bg.next()\n",
    "\n",
    "rnn = RNN(word_dict.vocabulary_size())\n",
    "state_size = rnn.state_size()\n",
    "saved_state = tf.Variable(\n",
    "            tf.zeros([batch_size, state_size]), trainable=False)\n",
    "\n",
    "outputs = rnn.forward_propagation(batches, saved_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100000)\n",
      "[u'boyt']\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "o = sess.run(outputs[-1])\n",
    "print o.shape\n",
    "print word_dict.word_from_probs(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading sentences from ../data/product_titles_100k.txt\n",
      "Parsed 100000 sentences.\n",
      "Maximum length of a sentence  1418\n",
      "Found 213667 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "valid_size = 100\n",
    "num_unrollings = 3\n",
    "\n",
    "random.seed(10)\n",
    "tokenized_sentences = tokenize_sentences_from_file(train_file)\n",
    "random.shuffle(tokenized_sentences, random.random)\n",
    "\n",
    "train_sent = tokenized_sentences[: -valid_size]\n",
    "valid_sent = tokenized_sentences[-valid_size: ]\n",
    "\n",
    "word_dict = WordDictionary(train_sent)\n",
    "train_data = word_dict.words_to_indices(train_sent)\n",
    "train_bg = BatchGenerator(train_data, word_dict, batch_size=batch_size,\n",
    "                          num_unrollings=num_unrollings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the network ready to run validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "class Validation(object):\n",
    "    def __init__(self, rnn, tokenized_sentences, word_dict):\n",
    "        print \"Number of validation sentences \", len(tokenized_sentences)\n",
    "        \n",
    "        self._sentences = tokenized_sentences\n",
    "        valid_data = word_dict.words_to_indices(tokenized_sentences)\n",
    "        \n",
    "        self._bg = BatchGenerator(valid_data, word_dict, batch_size=1,\n",
    "                          num_unrollings=1)\n",
    "        \n",
    "        vocabulary_size = word_dict.vocabulary_size()\n",
    "        \n",
    "        self._valid_x = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "        self._valid_y = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "        \n",
    "        state_size = rnn.state_size()\n",
    "        valid_state = tf.Variable(\n",
    "            tf.zeros([1, state_size]), trainable=False)\n",
    "        self._reset_valid_state = valid_state.assign(tf.zeros([1, state_size]))\n",
    "    \n",
    "        valid_logits, self._valid_state = rnn.rnn_cell(\n",
    "                self._valid_x, valid_state)\n",
    "        \n",
    "        self._valid_pred = tf.nn.softmax(valid_logits)\n",
    "        self._valid_loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    valid_logits, self._valid_y))\n",
    "        \n",
    "    def valid_metrics(self, sess):\n",
    "        num_tokens = 0\n",
    "        feed_dict = {}\n",
    "        valid_loss = 0.0\n",
    "        valid_logprob = 0.0\n",
    "\n",
    "        for sent in self._sentences:\n",
    "            sess.run(self._reset_valid_state)\n",
    "\n",
    "            for i in range(len(sent) + 1):\n",
    "                sentence_start, batch = self._bg.next()\n",
    "                if len(batch) != 2:\n",
    "                    print \"Failed batch length is not 2\", len(batch)\n",
    "\n",
    "                feed_dict[self._valid_x] = batch[0]\n",
    "                feed_dict[self._valid_y] = batch[1]\n",
    "                \n",
    "                p, l = sess.run([self._valid_pred, self._valid_loss], feed_dict=feed_dict)\n",
    "                \n",
    "                valid_logprob += logprob(p, batch[1])\n",
    "                \n",
    "                valid_loss += l\n",
    "                num_tokens += 1\n",
    "                \n",
    "        valid_loss /= num_tokens\n",
    "        perplexity = float(np.exp(valid_logprob / num_tokens))\n",
    "        \n",
    "        return valid_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation sentences  100\n",
      "Initialized\n",
      "Average loss at step 0: 11.601078 learning rate: 10.000000 time taken 1\n",
      "Validation set loss 11.135429, perplexity 68557.5454\n",
      "Average loss at step 50: 9.852473 learning rate: 10.000000 time taken 67\n",
      "Average loss at step 100: 8.466406 learning rate: 10.000000 time taken 57\n",
      "Average loss at step 150: 7.102902 learning rate: 10.000000 time taken 60\n",
      "Average loss at step 200: 6.890126 learning rate: 10.000000 time taken 55\n",
      "Average loss at step 250: 6.935275 learning rate: 10.000000 time taken 55\n",
      "Average loss at step 300: 6.489058 learning rate: 10.000000 time taken 54\n",
      "Average loss at step 350: 6.852758 learning rate: 10.000000 time taken 48\n",
      "Average loss at step 400: 6.690483 learning rate: 10.000000 time taken 48\n",
      "Average loss at step 450: 6.637797 learning rate: 10.000000 time taken 410\n",
      "Average loss at step 500: 6.646568 learning rate: 10.000000 time taken 54\n",
      "Validation set loss 9.216717, perplexity 10047.1748\n",
      "Average loss at step 550: 6.497103 learning rate: 10.000000 time taken 64\n",
      "Average loss at step 600: 6.284847 learning rate: 10.000000 time taken 54\n",
      "Average loss at step 650: 6.358456 learning rate: 10.000000 time taken 54\n",
      "Average loss at step 700: 6.659766 learning rate: 10.000000 time taken 54\n",
      "Average loss at step 750: 6.703174 learning rate: 10.000000 time taken 49\n",
      "Average loss at step 800: 6.531806 learning rate: 10.000000 time taken 57\n",
      "Average loss at step 850: 6.350218 learning rate: 10.000000 time taken 56\n",
      "Average loss at step 900: 6.128296 learning rate: 10.000000 time taken 56\n",
      "Average loss at step 950: 6.090811 learning rate: 10.000000 time taken 54\n",
      "Average loss at step 1000: 6.281371 learning rate: 10.000000 time taken 50\n",
      "Validation set loss 8.546114, perplexity 5146.7176\n",
      "Training Time 0:24:36.511163\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "state_size = 64\n",
    "summary_frequency = 50\n",
    "vocabulary_size = word_dict.vocabulary_size()\n",
    "\n",
    "# Train data.\n",
    "train_data = list()\n",
    "for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "        tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "saved_state = tf.Variable(\n",
    "            tf.zeros([batch_size, state_size]), trainable=False)\n",
    "reset_saved_state = saved_state.assign(tf.zeros([batch_size, state_size]))\n",
    "\n",
    "# Network and loss\n",
    "rnn = RNN(vocabulary_size, state_size = state_size)\n",
    "loss = rnn.loss(train_inputs, train_labels, saved_state)\n",
    "valid = Validation(rnn, valid_sent, word_dict)\n",
    "\n",
    "eval_saved_state = tf.Variable(\n",
    "            tf.zeros([1, state_size]), trainable=False)\n",
    "reset_eval_saved_state = eval_saved_state.assign(tf.zeros([1, state_size]))\n",
    "    \n",
    "# Optimizer.\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "# Run the session\n",
    "sess = tf.Session()\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "print('Initialized')\n",
    "\n",
    "mean_loss = 0\n",
    "\n",
    "train_start = time.time()\n",
    "step_start = time.time()\n",
    "\n",
    "for step in range(num_steps):\n",
    "    sentence_start, batches = train_bg.next()\n",
    "    if sentence_start:\n",
    "        sess.run(reset_saved_state)\n",
    "\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "    _, l, lr = sess.run(\n",
    "        [optimizer, loss, learning_rate], feed_dict=feed_dict)\n",
    "    \n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "        step_time = time.time() - step_start\n",
    "        step_start = time.time()\n",
    "        if step > 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "        print('Average loss at step %d: %f learning rate: %f time taken %d' % (\n",
    "                step, mean_loss, lr, step_time))\n",
    "        mean_loss = 0\n",
    "            \n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "            v_loss, v_perplex = valid.valid_metrics(sess)\n",
    "            print \"Validation set loss %f, perplexity %.4f\" % (v_loss, v_perplex)\n",
    "\n",
    "train_time = time.time() - train_start\n",
    "print \"Training Time \" + str(datetime.timedelta(seconds=train_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction for a sample sentence\n",
    "Predicts the next tokens from the previous tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of a sentence  34\n",
      "[['Edward', 'Osbaldeston', 'Religious', 'Medal']]\n",
      "5\n",
      "6\n",
      "[['UNKNOWN_TOKEN'], ['UNKNOWN_TOKEN'], [u'of'], [u'got'], [u\"'s\"]]\n",
      "Input:  Edward Osbaldeston Religious Medal\n",
      "Output:  UNKNOWN_TOKEN UNKNOWN_TOKEN of got 's\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Edward Osbaldeston Religious Medal\"\n",
    "\n",
    "tokenized_sentences = tokenize_sentences([sentence])\n",
    "sent_indices = word_dict.words_to_indices(tokenized_sentences)\n",
    "\n",
    "print tokenized_sentences\n",
    "num_unrollings = len(tokenized_sentences[0]) + 1\n",
    "print num_unrollings\n",
    "\n",
    "input_data = list()\n",
    "for _ in range(num_unrollings):\n",
    "    input_data.append(\n",
    "        tf.placeholder(tf.float32, shape=[1, word_dict.vocabulary_size()]))\n",
    "\n",
    "bg = BatchGenerator(sent_indices, word_dict, batch_size=1,\n",
    "                    num_unrollings=num_unrollings)\n",
    "_, batch = bg.next()\n",
    "print len(batch)\n",
    "\n",
    "outputs = rnn.forward_propagation(input_data, eval_saved_state)\n",
    "\n",
    "feed_dict = {}\n",
    "for i in range(num_unrollings):\n",
    "    feed_dict[input_data[i]] = batch[i]\n",
    "\n",
    "sess.run(reset_eval_saved_state)\n",
    "preds = sess.run(outputs, feed_dict=feed_dict)\n",
    "    \n",
    "output = [word_dict.word_from_probs(p) for p in preds]\n",
    "print output\n",
    "\n",
    "print \"Input: \", ' '.join(tokenized_sentences[0])\n",
    "print \"Output: \", ' '.join([o[0] for o in output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

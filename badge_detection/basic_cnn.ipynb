{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cPickle as pickle\n",
    "import sklearn.metrics as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(predictions, labels):\n",
    "    pred = np.argmax(predictions, 1)\n",
    "    actual = np.argmax(labels, 1)\n",
    "    \n",
    "    print \"Actuals \", np.bincount(actual)\n",
    "    print \"Predictions \", np.bincount(pred)\n",
    "    \n",
    "    print \"Accuracy \", 100.0 * np.sum(pred == actual) / pred.shape[0]\n",
    "    print \"Two class confusion matrix\"\n",
    "    p = (pred != 0)\n",
    "    a = (actual != 0)\n",
    "    print sm.confusion_matrix(p, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (2000, 256, 256, 3), (2000,))\n",
      "('Validation set', (250, 256, 256, 3), (250,))\n",
      "('Test set', (250, 256, 256, 3), (250,))\n",
      "('Custom Test set', (57, 256, 256, 3), (57,))\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'badged_images.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "\n",
    "  custom_test_dataset = save['custom_test_dataset']\n",
    "  custom_test_labels = save['custom_test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Custom Test set', custom_test_dataset.shape, custom_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_labels = np.unique(train_labels).shape[0]\n",
    "image_height = train_dataset.shape[1]\n",
    "image_width = train_dataset.shape[2]\n",
    "num_channels = 1\n",
    "\n",
    "if len(train_dataset.shape) > 3:\n",
    "    num_channels = train_dataset.shape[3]\n",
    "\n",
    "def reformat(dataset):\n",
    "  return dataset.reshape(\n",
    "    (-1, image_height, image_width, num_channels)).astype(np.float32)\n",
    "\n",
    "def one_hot_encoding(labels):\n",
    "  return (np.arange(num_labels) == labels[:,None]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 256 256\n",
      "('Training set', (2000, 256, 256, 3), (2000, 2))\n",
      "('Validation set', (250, 256, 256, 3), (250, 2))\n",
      "('Test set', (250, 256, 256, 3), (250, 2))\n",
      "('Custom Test set', (250, 256, 256, 3), (57, 2))\n"
     ]
    }
   ],
   "source": [
    "print num_labels, image_height, image_width\n",
    "\n",
    "train_dataset = reformat(train_dataset)\n",
    "valid_dataset = reformat(valid_dataset)\n",
    "test_dataset = reformat(test_dataset)\n",
    "custom_test_dataset = reformat(custom_test_dataset)\n",
    "\n",
    "train_labels = one_hot_encoding(train_labels)\n",
    "valid_labels = one_hot_encoding(valid_labels)\n",
    "test_labels = one_hot_encoding(test_labels)\n",
    "custom_test_labels = one_hot_encoding(custom_test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "print('Custom Test set', custom_test_dataset.shape, custom_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation baseline accuracy\n",
      "Actuals  [150 100]\n",
      "Predictions  [250]\n",
      "Accuracy  60.0\n",
      "Two class confusion matrix\n",
      "[[150 100]\n",
      " [  0   0]]\n",
      "Test baseline accuracy\n",
      "Actuals  [156  94]\n",
      "Predictions  [250]\n",
      "Accuracy  62.4\n",
      "Two class confusion matrix\n",
      "[[156  94]\n",
      " [  0   0]]\n"
     ]
    }
   ],
   "source": [
    "print \"Validation baseline accuracy\"\n",
    "confusion_matrix(one_hot_encoding(np.zeros(valid_labels.shape[0])), valid_labels)\n",
    "\n",
    "print \"Test baseline accuracy\"\n",
    "confusion_matrix(one_hot_encoding(np.zeros(test_labels.shape[0])), test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_height, image_width, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  tf_custom_test_dataset = tf.constant(custom_test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_height // 4 * image_width // 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  def model(data):\n",
    "    h_conv1 = tf.nn.relu(conv2d(data, layer1_weights) + layer1_biases)\n",
    "    h_pool1 = max_pool_2x2(h_conv1) \n",
    "    \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_pool1, layer2_weights) + layer2_biases)\n",
    "    h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "    shape = h_pool2.get_shape().as_list()\n",
    "    reshape = tf.reshape(h_pool2, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = model(tf_train_dataset)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.00005).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "  test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "  custom_test_prediction = tf.nn.softmax(model(tf_custom_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 918.556152\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 60.0%\n",
      "Actuals  [150 100]\n",
      "Predictions  [250]\n",
      "Accuracy  60.0\n",
      "Two class confusion matrix\n",
      "[[150 100]\n",
      " [  0   0]]\n",
      "Minibatch loss at step 50: 1.758260\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 60.0%\n",
      "Actuals  [150 100]\n",
      "Predictions  [166  84]\n",
      "Accuracy  60.0\n",
      "Two class confusion matrix\n",
      "[[108  58]\n",
      " [ 42  42]]\n",
      "Minibatch loss at step 100: 0.705586\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 60.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [191  59]\n",
      "Accuracy  60.4\n",
      "Two class confusion matrix\n",
      "[[121  70]\n",
      " [ 29  30]]\n",
      "Minibatch loss at step 150: 1.366789\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 40.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [ 43 207]\n",
      "Accuracy  40.4\n",
      "Two class confusion matrix\n",
      "[[ 22  21]\n",
      " [128  79]]\n",
      "Minibatch loss at step 200: 0.542827\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 58.0%\n",
      "Actuals  [150 100]\n",
      "Predictions  [205  45]\n",
      "Accuracy  58.0\n",
      "Two class confusion matrix\n",
      "[[125  80]\n",
      " [ 25  20]]\n",
      "Minibatch loss at step 250: 5.180737\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 59.6%\n",
      "Actuals  [150 100]\n",
      "Predictions  [235  15]\n",
      "Accuracy  59.6\n",
      "Two class confusion matrix\n",
      "[[142  93]\n",
      " [  8   7]]\n",
      "Minibatch loss at step 300: 1.655782\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 59.6%\n",
      "Actuals  [150 100]\n",
      "Predictions  [221  29]\n",
      "Accuracy  59.6\n",
      "Two class confusion matrix\n",
      "[[135  86]\n",
      " [ 15  14]]\n",
      "Minibatch loss at step 350: 1.171422\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 38.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [ 32 218]\n",
      "Accuracy  38.4\n",
      "Two class confusion matrix\n",
      "[[ 14  18]\n",
      " [136  82]]\n",
      "Minibatch loss at step 400: 0.852619\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 60.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [229  21]\n",
      "Accuracy  60.4\n",
      "Two class confusion matrix\n",
      "[[140  89]\n",
      " [ 10  11]]\n",
      "Minibatch loss at step 450: 1.009693\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 59.6%\n",
      "Actuals  [150 100]\n",
      "Predictions  [231  19]\n",
      "Accuracy  59.6\n",
      "Two class confusion matrix\n",
      "[[140  91]\n",
      " [ 10   9]]\n",
      "Minibatch loss at step 500: 0.694134\n",
      "Minibatch accuracy: 56.2%\n",
      "Validation accuracy: 39.6%\n",
      "Actuals  [150 100]\n",
      "Predictions  [ 27 223]\n",
      "Accuracy  39.6\n",
      "Two class confusion matrix\n",
      "[[ 13  14]\n",
      " [137  86]]\n",
      "Minibatch loss at step 550: 0.625751\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 59.2%\n",
      "Actuals  [150 100]\n",
      "Predictions  [232  18]\n",
      "Accuracy  59.2\n",
      "Two class confusion matrix\n",
      "[[140  92]\n",
      " [ 10   8]]\n",
      "Minibatch loss at step 600: 1.759646\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 60.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [227  23]\n",
      "Accuracy  60.4\n",
      "Two class confusion matrix\n",
      "[[139  88]\n",
      " [ 11  12]]\n",
      "Minibatch loss at step 650: 0.621687\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 60.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [235  15]\n",
      "Accuracy  60.4\n",
      "Two class confusion matrix\n",
      "[[143  92]\n",
      " [  7   8]]\n",
      "Minibatch loss at step 700: 1.216215\n",
      "Minibatch accuracy: 31.2%\n",
      "Validation accuracy: 59.6%\n",
      "Actuals  [150 100]\n",
      "Predictions  [243   7]\n",
      "Accuracy  59.6\n",
      "Two class confusion matrix\n",
      "[[146  97]\n",
      " [  4   3]]\n",
      "Minibatch loss at step 750: 0.938115\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 58.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [241   9]\n",
      "Accuracy  58.8\n",
      "Two class confusion matrix\n",
      "[[144  97]\n",
      " [  6   3]]\n",
      "Minibatch loss at step 800: 1.083910\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 59.6%\n",
      "Actuals  [150 100]\n",
      "Predictions  [237  13]\n",
      "Accuracy  59.6\n",
      "Two class confusion matrix\n",
      "[[143  94]\n",
      " [  7   6]]\n",
      "Minibatch loss at step 850: 0.537256\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 59.2%\n",
      "Actuals  [150 100]\n",
      "Predictions  [236  14]\n",
      "Accuracy  59.2\n",
      "Two class confusion matrix\n",
      "[[142  94]\n",
      " [  8   6]]\n",
      "Minibatch loss at step 900: 0.643247\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 38.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [ 17 233]\n",
      "Accuracy  38.8\n",
      "Two class confusion matrix\n",
      "[[  7  10]\n",
      " [143  90]]\n",
      "Minibatch loss at step 950: 2.830944\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 59.6%\n",
      "Actuals  [150 100]\n",
      "Predictions  [245   5]\n",
      "Accuracy  59.6\n",
      "Two class confusion matrix\n",
      "[[147  98]\n",
      " [  3   2]]\n",
      "Minibatch loss at step 1000: 0.992527\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 58.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [239  11]\n",
      "Accuracy  58.8\n",
      "Two class confusion matrix\n",
      "[[143  96]\n",
      " [  7   4]]\n",
      "Minibatch loss at step 1050: 0.655549\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 58.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [241   9]\n",
      "Accuracy  58.8\n",
      "Two class confusion matrix\n",
      "[[144  97]\n",
      " [  6   3]]\n",
      "Minibatch loss at step 1100: 0.725479\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 58.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [239  11]\n",
      "Accuracy  58.8\n",
      "Two class confusion matrix\n",
      "[[143  96]\n",
      " [  7   4]]\n",
      "Minibatch loss at step 1150: 1.463295\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 40.0%\n",
      "Actuals  [150 100]\n",
      "Predictions  [ 14 236]\n",
      "Accuracy  40.0\n",
      "Two class confusion matrix\n",
      "[[  7   7]\n",
      " [143  93]]\n",
      "Minibatch loss at step 1200: 0.789677\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 58.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [243   7]\n",
      "Accuracy  58.8\n",
      "Two class confusion matrix\n",
      "[[145  98]\n",
      " [  5   2]]\n",
      "Minibatch loss at step 1250: 0.763510\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 58.0%\n",
      "Actuals  [150 100]\n",
      "Predictions  [237  13]\n",
      "Accuracy  58.0\n",
      "Two class confusion matrix\n",
      "[[141  96]\n",
      " [  9   4]]\n",
      "Minibatch loss at step 1300: 0.848470\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 40.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [ 15 235]\n",
      "Accuracy  40.4\n",
      "Two class confusion matrix\n",
      "[[  8   7]\n",
      " [142  93]]\n",
      "Minibatch loss at step 1350: 0.696837\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 40.0%\n",
      "Actuals  [150 100]\n",
      "Predictions  [ 24 226]\n",
      "Accuracy  40.0\n",
      "Two class confusion matrix\n",
      "[[ 12  12]\n",
      " [138  88]]\n",
      "Minibatch loss at step 1400: 0.607606\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 58.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [239  11]\n",
      "Accuracy  58.8\n",
      "Two class confusion matrix\n",
      "[[143  96]\n",
      " [  7   4]]\n",
      "Minibatch loss at step 1450: 0.788476\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 58.8%\n",
      "Actuals  [150 100]\n",
      "Predictions  [229  21]\n",
      "Accuracy  58.8\n",
      "Two class confusion matrix\n",
      "[[138  91]\n",
      " [ 12   9]]\n",
      "Minibatch loss at step 1500: 1.073359\n",
      "Minibatch accuracy: 37.5%\n",
      "Validation accuracy: 58.4%\n",
      "Actuals  [150 100]\n",
      "Predictions  [236  14]\n",
      "Accuracy  58.4\n",
      "Two class confusion matrix\n",
      "[[141  95]\n",
      " [  9   5]]\n",
      "Test accuracy: 62.8%\n",
      "Actuals  [156  94]\n",
      "Predictions  [225  25]\n",
      "Accuracy  62.8\n",
      "Two class confusion matrix\n",
      "[[144  81]\n",
      " [ 12  13]]\n",
      "Custom Test accuracy: 57.9%\n",
      "Actuals  [30 27]\n",
      "Predictions  [54  3]\n",
      "Accuracy  57.8947368421\n",
      "Two class confusion matrix\n",
      "[[30 24]\n",
      " [ 0  3]]\n",
      "CPU times: user 2h 3min 40s, sys: 22min 9s, total: 2h 25min 50s\n",
      "Wall time: 31min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "num_steps = 1501\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "    \n",
    "      valid_preds = valid_prediction.eval()\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_preds, valid_labels))\n",
    "      confusion_matrix(valid_preds, valid_labels)\n",
    "\n",
    "  test_preds = test_prediction.eval()\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_preds, test_labels))\n",
    "  confusion_matrix(test_preds, test_labels)\n",
    "\n",
    "  custom_test_preds = custom_test_prediction.eval()\n",
    "  print('Custom Test accuracy: %.1f%%' % accuracy(custom_test_preds, custom_test_labels))\n",
    "  confusion_matrix(custom_test_preds, custom_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
